\chapter{Методы многомерной минимизации}
\section{Основные определения}
Далее будем говорить про функции многих переменных, то есть однозначное соответствие $f(\vec x) = y$. Здесь $y$ - скаляр, а $\vec x$ - вектор столбец:
\begin{equation}
    \vec x = \left[
    \begin{array}{c}
         x_1  \\
         x_2  \\
         \vdots \\
         x_n
    \end{array}
    \right]
\end{equation}

Положительно определенной матрицей будем называть симметричную квадратную матрицу $a^T=a$, такую, что $f(\vec x) = {\vec x}^Ta{\vec x} > 0$ для любого $\vec x \neq \vec 0$. Для того, чтобы матрица была положительно определенной необходимо и достаточно, чтобы все собственные значения $\lambda$ данной матрицы были положительны, то есть при решении уравнения:
\begin{equation}
    \det \left( a_{ik} - \lambda\delta_{ik} \right)=0
\end{equation}
Здесь $\delta_{ik}$ - символ Кронекера, то есть:
\begin{equation*}
\delta_{ik} =
\left\{
\begin{array}{lr}
1 & \text{ для } i = k\\
0 & \text{ для } i \neq k
\end{array}
\right.
\end{equation*}
Пусть в многомерном пространстве $\vec x_0$ - первая точка, то есть точка с которой мы начинаем поиск минимума, а $\vec x^*$ - точка минимума функции. Алгоритм многомерной минимизации должен обеспечивать:
\begin{itemize}
    \item Направление движения к $\vec x^*$
    \item Задавать длину шага
\end{itemize}
Одной формулой требования к алгоритму можно задать так:
\begin{equation}
    \vec x_{k+1} =\vec x_k + \alpha_k \vec p
    \label{xk}
\end{equation}
Здесь $ \vec x_{k+1}$ следующая точка, получаемая с помощью алгоритма, $ \vec x_{k}$ - текущая точка, с которой производится переход в $ \vec x_{k+1}$, $\alpha_k$ - длина шага на $k$-ом этапе (скаляр) и $\vec p_k = \vec x_{k+1} - \vec x_{k} $ - вектор направления.

Гипотетически наша функция имеет аналитический вид, поэтому мы можем разложить ее в ряд Тейлора в некоторой окрестности точки $\vec x_k$:
\begin{equation}
    f(\vec x) = f(\vec x_k) + \alpha(f_{kc}',\vec p_k) + \frac{\alpha^2}{2}(f_{kc}'' \, \vec p_k,\vec p_k)
    \label{Teylor}
\end{equation}
Здесь стоит расписать обозначения, применимые в формуле (\ref{Teylor}):
\begin{equation*}
    f'_{kc} = \left[ 
    \begin{array}{c}
         \left.\frac{\partial f(\vec x)}{\partial x_1}\right|_{x_{kc_1}} \\
         \left.\frac{\partial f(\vec x)}{\partial x_2}\right|_{x_{kc_2}} \\
         \vdots \\
         \left.\frac{\partial f(\vec x)}{\partial x_n}\right|_{x_{kc_n}}
         \end{array}
    \right]^T
\end{equation*}
\begin{equation*}
    f_{kc}'' = \left[ 
    \begin{array}{cccc}
         \left.\frac{\partial^2 f(\vec x)}{\partial x_1^2}\right|_{x_{kc_1}} &
         \left.\frac{\partial^2 f(\vec x)}{\partial x_1 \partial x_2}\right|_{x_{kc_1}} &
         \ldots &  \left.\frac{\partial^2 f(\vec x)}{\partial x_1 \partial x_n}\right|_{x_{kc_1}} \\
         \left.\frac{\partial^2 f(\vec x)}{\partial x_2 \partial x_1}\right|_{x_{kc_2}} &
         \left.\frac{\partial^2 f(\vec x)}{\partial x_2^2}\right|_{x_{kc_2}} &
         \ldots &  \left.\frac{\partial^2 f(\vec x)}{\partial x_2 \partial x_n}\right|_{x_{kc_2}} \\
         \vdots & \vdots & \ddots & \vdots \\
         \left.\frac{\partial^2 f(\vec x)}{\partial x_n \partial x_1}\right|_{x_{kc_n}} &
         \left.\frac{\partial^2 f(\vec x)}{\partial x_n \partial x_2}\right|_{x_{kc_n}} &
         \ldots &  \left.\frac{\partial^2 f(\vec x)}{\partial x_n^2}\right|_{x_{kc_n}}
    \end{array}
    \right]
\end{equation*}
Дифференцирование в точке $\vec x_k$ фактически означает дифференцирование в окрестности, близ точки  $\vec x_k$, то есть типа в интервале $[x_k, x_{kc}]$, где $x_{kc}$ точка, значение которой дается формулой:
\begin{equation*}
   \vec x_{kc} =\vec x_k + \theta\left(\vec x - \vec x_k\right) \qquad \theta \epsilon [0; 1]
\end{equation*}

Так как мы ищем минимум при условии, что движение направлено к точке минимума, то $f(\vec x_{min}) < f(\vec x_k) < f(\vec x_0)$, где $\vec x_{min}$ - точка фактического минимума функции. 

Считаем, что разложение происходит настолько близко к точке $\vec x_{min}$, что функция является невыпуклой, а значит матрица второй производной положительно определена, что дает нам сделать утверждение о знаке второго члена разложения ряда (\ref{Teylor}). Поскольку, как мы говорили выше, для положительно определенной матрицы $\left(f_{kc}'' \, \vec p_k,\vec p_k\right) > 0$, а $\frac{\alpha^2}{2} \geqslant 0$, то получается, что слагаемое второго порядка положительно. Но так как есть необходимость в том, чтобы в точке, ближней к точке фактического минимума функции ($f(\vec x) < f(\vec x_k)$), то слагаемое первого порядка $\alpha(f_{kc}',\vec p_k) < 0$, что подводит нас к первому методу многомерной минимизации.

\section{Градиентный метод} \label{GradientMethod}
В данном методе в качестве $\vec p_k$ просто берется выражение $-f'(\vec x_k) = -f'_k$. Тогда выражение (\ref{xk}) примет вид:
\begin{equation}
    \vec x_{k+1} = \vec x_k - \alpha_k f'(\vec x_k) \textbf{, } \quad \alpha_k > 0 \text{, } k = 0, 1 \ldots
\end{equation}
Выбором такого значения $\vec p_k$ мы определили направление движения, а также уменьшения функции при приближении к точке $(f'(\vec x_k), p) < 0$. Осталось найти шаг $\alpha_k$, где $k$ -индекс итерации.

Рассмотрим процесс выбора шага на примере.
Пусть рассматриваемая функция имеет аналитическое выражение:\
\begin{equation}
    f(x,y) = \frac12\left(\frac{x^2}{a^2} + \frac{y^2}{b^2}\right)
    \label{ellipse}
\end{equation}
\input{Pictures/Antigradient}
На рисунке \ref{fig:antigradient} разными цветами показаны линии уровня эллипсоида. Линия уровня соответствует какому-то значению функции во всех ее точках. Так, значение в красном сечении эллипсоида больше чем в синем, а то, в свою очередь, больше чем в зеленом. Предположим, что мы выбрали точку $(x_0, y_0)$ на поверхности эллипсоида. Проведем линию уровня в этой точке, то есть линию, которая получалась бы, если бы мы <<отрезали>> кусок эллипсоида в этой точке перпендикулярно оси значений функции ($f(x,y)$). Далее в этой линии уровня проводим касательную в этой точке. Градиент функции в этой точки всегда будет перпендикулярен касательной. Так как метод призван искать минимум функции, то направление движения стоит выбирать к минимуму, а значит со знаком минус. На рисунке коричневой стрелкой показано направление движения в поисках минимума. 

Как же выбрать шаг? В сечении направления движения мы получаем кривую второго порядка. Останавливаться стоит тогда, когда достигнут минимум этой кривой.. То есть, допустим мы рассекли эллипсоид и получили нечто вроде параболы в этом сечении. Значит минимум этой параболы и будет следующей точкой, где будет повторяться описанная выше процедура. На рисунке \ref{fig:antigradient} эта точка обозначена $(x_1, y_1)$. После ее нахождения получаем новую линию уровня, проводим новую касательную и т. д. Процесс повторяется до тех пор, пока будет условие, где $\varepsilon$ мы задаем сами.:
\begin{equation}
    \frac{f(\vec x_{k+1}) - f(\vec x_k)}{f(\vec x_k)} > \varepsilon
    \label{ag_stop}
\end{equation}
 
\section*{Вопрос}
В каком случае метод антиградиента справится с функцией \ref{ellipse} за один шаг, а в каком, возможно, вовсе не справится?
\section*{Ответ}
В случае если $a=b$. Тогда, полученная поверхность это сфера. При получении любой линии уровня - в сечении будет окружность, а значит направление антиградиента всегда будет к центру этой окружности. Сечение, проходящее через центр окружности, будет пересекать и минимум сферы, а значит первый же шаг приведет нас в минимум функции.

В случае $a\ll b$ или $a \gg b$ одна ось эллипсоида будет вытянута относительно другой достаточно сильно. Если мы начнем наши измерения с точки, далекой от центра и антиградент в которой не будет сразу проходить через центр, то мы будем идти к минимуму достаточно долго, или условие (\ref{ag_stop}) выполнится гораздо раньше, чем мы достигнем минимума.

На этом примере мы видим, что метод антиградиента не так хорош, однако, путем замены координат, этот случай можно свести к обыкновенной сфере, с чем, как мы убедились, градиентный метод с легкостью справится. Тут возникает одно из условий для использования метода: Необходимо, чтобы масштаб осей был соразмерным, то есть изменение по одной оси на 1 не приводило к изменению по другой оси на, скажем, 1000. 

Вообще говоря, главный недостаток метода антиградента состоит в том, что он не чувствителен к перпендикулярным направлениям. То есть сечение функции по направлению наибольшего спада не учитывает значения функции в некоторой окрестности кривой сечения. Однако следующий метод лишен данного недостатка.


\input{Multidimensional minimization methods/MMM_NewtMeth}
\input{Multidimensional minimization methods/MMM_DavidonMeth}
\input{Multidimensional minimization methods/MMM_comparison}
