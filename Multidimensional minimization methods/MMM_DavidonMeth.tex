\section{Метод Дэвидона-Флетчера-Пауэлла}
Мы увидели, что в отличие от метода антиградиента, метод Ньютона <<Умеет смотреть по сторонам>>, путям взятия матрицы второй производной, а значит определения выпуклости функции в точке. Однако, учитывая специфику данного метода, существует необходимость взятия обратной матрицы от матрицы вторых производных (матрицы Гессе), что, вообще говоря, не всегда является корректной задачей, поскольку процедура обращения матрицы неустойчива. Поэтому возникла идея создать такой метод, в которым нет необходимости вычислятьобратную матрицу Гессе.

Алгоритм автоматически синтезирует метод антиградиента и метод Ньютона. Происходит это за счет того, что процедура поиска минимума начинается градиентным методом, описанным в подразделе \ref{GradientMethod}, и заканчивается методом Ньютона (\ref{NewtMeth}).
В этом случае выражение \ref{xk} имеет следующий вид:
\begin{equation}
    \vec x_{k+1} = \vec x_k - \lambda_k \eta(\vec x_k)f'(\vec x_k)
\end{equation}
где $\eta(\vec x_k)$ - матрица направлений, $\lambda_k$ - длина шага.
Сущность метода антиградиента проявляется только на первом шаге. В этот момент матрица направлений эквивалентна единичной матрице, размерности $n$, где количество параметров функции:
\begin{equation*}
    \eta (x_0) = \hat I = \left[\begin{array}{cccc}
        1 & 0 & \ldots & 0 \\
        0 & 1 & \ldots & 0 \\
        \vdots & \vdots & \ddots & \vdots\\
        0 & 0 & \ldots & 1
    \end{array}\right]
\end{equation*}
А в точке минимума матрица направлений должна совпадать с матрицей обратной матрице Гессе, то есть:
\begin{equation*}
    \eta(x_{min}) = \left(f''(x_{min})\right)^{-1}
\end{equation*}
Вторая производная в дискретных функциях может вычисляться следующим образом, поскольку мы не располагаем аналитическим видом функции, а значит не можем пользоваться классическим определением производной через пределы:
\begin{equation}
    f'(\vec x_{k+1})-f'(\vec x_k) \approx f''(\vec x_k)(\vec x_{k+1}-\vec x_k)
    \label{deriv}
\end{equation}
Домножаем (\ref{deriv}) слева на обратную матрицу Гессе и получаем:
\begin{equation}
    \Delta \vec x_k=\vec x_{k+1}-\vec x_k \approx \left(f''(\vec x_k)\right)^{-1}\left(f'(\vec x_{k+1})-f'(\vec x_k)\right) =  \left(f''(\vec x_k)\right)^{-1}\Delta g_k
\label{delta_x}
\end{equation}
Хотим, чтобы $ \left(f''(\vec x_{k+1})\right)^{-1}$ была пропорциональна матрице направлений на $k+1$ шаге. Более того, хотим, чтобы матрица направлений менялась от шага к шагу аддитивно, то есть:
\begin{equation}
    (f''(\vec x_{k+1}))^{-1} \approx w\eta(\vec x_{k+1}) \approx w(\eta(\vec x_k)+\Delta \eta (\vec x_k))
    \label{ita}
\end{equation}
Здесь $\omega$ - коэффициент пропорциональности. Тогда, учитывая (\ref{delta_x}) и (\ref{ita}):
\begin{align}
    \eta_{k+1}\Delta g_k & {}= \frac1{\omega}\Delta \vec x_k \\
    \Delta \eta_{k} \Delta g_k & {} =
    \frac1{\omega} \Delta \vec x_k - \eta_k \Delta g_k
    \label{delta_eta}\end{align}
Если рассматривать это выражение как уравнение относительно $\Delta \eta_k$, то его решение:
\begin{equation}
    \Delta \eta_k = \frac1{\omega}\frac{\Delta\vec x_k {\vec y}^T}{{\vec y}^T \Delta g_k} - \frac{\eta_k \Delta g_k {\vec z}^T}{{\vec z}^T \Delta g_k}
    \label{solve_eta}
\end{equation}
Здесь $\vec z$ и $\vec y$ - произвольные вектора.

Если решение (\ref{solve_eta}) подставить в (\ref{delta_eta}), то получим тождество. Поскольку вектора $\vec z$ и $\vec y$ произвольные, то можем положить:
\begin{equation}
    \vec y_k = \Delta \vec x_k \text{;} \qquad \vec z_k = \eta_k \Delta g_k
\end{equation}
Чтобы $\eta_{k+1}$ получилась равной обратной матрице Гессе на $k$-м шаге.

Напишем алгоритм для вычисления минимума методом Дэвидона.

Чтобы программа по нахождению минимума остановилась, нужно, чтобы одновременно выполнились следующие условия:
\begin{equation}
    \frac{f(\vec x_{k+1}) - f(\vec x_k)}{f(\vec x_k)} < \varepsilon_1
\end{equation}
\begin{equation}
    \frac{\Delta \vec x_i^{(k)}}{\vec x_i^{(k)}} < \varepsilon_{2}^{(i)}
\end{equation}